{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "softlab-full.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jay22519/Automatic-Answer-checker-/blob/main/softlab_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54CPRXGfyg0h"
      },
      "source": [
        "#NECESSARY IMPORTS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4hbFozwb2M5"
      },
      "source": [
        "%%time\n",
        "\n",
        "# For OCR\n",
        "%pip install -q autocorrect\n",
        "%pip install -q azure-cognitiveservices-vision-computervision\n",
        "%pip install -q pillow\n",
        "\n",
        "from autocorrect import Speller\n",
        "from azure.cognitiveservices.vision.computervision import ComputerVisionClient\n",
        "from azure.cognitiveservices.vision.computervision.models import OperationStatusCodes, VisualFeatureTypes\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from getpass import getpass\n",
        "\n",
        "subscription_key = getpass(\"Enter the Azure API key: \")\n",
        "endpoint = \"https://autoanschecker.cognitiveservices.azure.com/\"\n",
        "computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))\n",
        "\n",
        "# For NLP\n",
        "%pip install -q gensim\n",
        "%pip install -q nltk\n",
        "%pip install -q sklearn\n",
        "%pip install -q tensorflow_hub\n",
        "%pip install -q pyemd\n",
        "\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "from pyemd import emd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97QBu3vV8fhQ"
      },
      "source": [
        "# OCR code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK6zDLbmK95t"
      },
      "source": [
        "\"\"\"\n",
        "Takes a list of uploaded filenames and returns list of file_text\n",
        "\"\"\"\n",
        "def get_ocr_text(uploaded_files):\n",
        "    to_return = []\n",
        "\n",
        "    if not uploaded_files:\n",
        "        print(\"No files uploaded!\")\n",
        "    \n",
        "    check = Speller(lang='en')\n",
        "    for i, image in enumerate(uploaded_files, 1):\n",
        "        # Open the image\n",
        "        local_image_handwritten = open(image, \"rb\")\n",
        "\n",
        "        # Call API with image and raw response (allows you to get the operation location)\n",
        "        recognize_handwriting_results = computervision_client.read_in_stream(local_image_handwritten, raw=True)\n",
        "\n",
        "        # Get the operation location (URL with ID as last appendage)\n",
        "        operation_location_local = recognize_handwriting_results.headers[\"Operation-Location\"]\n",
        "        # Take the ID off and use to get results\n",
        "        operation_id_local = operation_location_local.split(\"/\")[-1]\n",
        "\n",
        "        # Call the \"GET\" API and wait for the retrieval of the results\n",
        "        while True:\n",
        "            recognize_handwriting_result = computervision_client.get_read_result(operation_id_local)\n",
        "            if recognize_handwriting_result.status not in ['notStarted', 'running']:\n",
        "                break\n",
        "            time.sleep(1)\n",
        "\n",
        "        # print(f\"===== Extracted text from image #{i}: =====\")\n",
        "        lines = []\n",
        "        # Print results, line by line\n",
        "        if recognize_handwriting_result.status == OperationStatusCodes.succeeded:\n",
        "            for text_result in recognize_handwriting_result.analyze_result.read_results:\n",
        "                for line in text_result.lines:\n",
        "                    # print(line.text, sep=' ')        # original OCR'ed line\n",
        "                    # only autocorrect words which aren't abbreviations.\n",
        "                    corrected = [word if bool(re.search(\"[A-Z]+\", word)) else check(word) for word in line.text.split() ]\n",
        "                    # print(\" \".join(corrected))\n",
        "                    lines.append(\" \".join(corrected))\n",
        "\n",
        "        to_return.append(\" \".join(lines))\n",
        "        # print()\n",
        "    return to_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpcNsdzSb4Hm"
      },
      "source": [
        "# NLP Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2BoN8HsegiN"
      },
      "source": [
        "%%time\n",
        "path_to_saved_model = gensim.downloader.load('word2vec-google-news-300', return_path=True)\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format(path_to_saved_model, binary=True)  \n",
        "model.init_sims(replace=True)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVPJTPB2IVXW"
      },
      "source": [
        "path_to_saved_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUv8i9ONru5x"
      },
      "source": [
        "%%time\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH5Ee_FmsAhp"
      },
      "source": [
        "def cos_sim(input_vectors):\n",
        "    similarity = cosine_similarity(input_vectors)\n",
        "    return similarity\n",
        "\n",
        "negative = [\"not\" , \"without\", \"against\", \"bad\", \"useless\", \"no\", \"dislike\", \"hate\"]\n",
        "\n",
        "def semantic_similarity(actual_answer , given_answer) :\n",
        "    actual = actual_answer.lower().split(\".\")\n",
        "    given = given_answer.lower().split(\".\")\n",
        "    \n",
        "    sim_checker = actual \n",
        "    \n",
        "    not_matching_semantics = list()\n",
        "    \n",
        "    semantic_1 = 0   # Actual_answer\n",
        "    semantic_2 = 0   # Given_answer\n",
        "    \n",
        "    actual_embed_list = list()\n",
        "    given_embed_list = list()\n",
        "    \n",
        "    for z in range(len(actual)) :\n",
        "        list_actual = list()  \n",
        "        list_actual.append(actual[z])\n",
        "        actual_embed_list.append(embed(list_actual))\n",
        "        #print(actual_embed_list[z].shape)\n",
        "    \n",
        "    for z in range(len(given)) :    \n",
        "        semantic_1 = 0\n",
        "        semantic_2 = 0 \n",
        "        list_given = list()\n",
        "        list_given.append(given[z])\n",
        "        embed_z = embed(list_given)\n",
        "        \n",
        "        sim_check = sim_checker.copy() \n",
        "        sim_check.append(given[z]) \n",
        "        \n",
        "        sen_em = embed(sim_check)\n",
        "        \n",
        "        similarity_matrix = cos_sim(np.array(sen_em))\n",
        "        \n",
        "        similarity_matrix_df = pd.DataFrame(similarity_matrix) \n",
        "        \n",
        "        cos_list = list(similarity_matrix_df[len(similarity_matrix_df) - 1]) \n",
        "        cos_list = cos_list[:len(cos_list)-1]\n",
        "        #print(cos_list)\n",
        "        \n",
        "        index = cos_list.index(max(cos_list))\n",
        "        \n",
        "        actual_check = actual[index]\n",
        "        actual_check = actual_check.split()\n",
        "        for i in range(len(actual_check) - 1) :\n",
        "            if(actual_check[i] in negative and actual_check[i+1] in negative) :\n",
        "                semantic_1 += 1 \n",
        "            elif(actual_check[i] in negative and actual_check[i+1] not in negative) :\n",
        "                semantic_1 -= 1\n",
        "\n",
        "        answer_given = given[z].split()\n",
        "        for i in range(len(answer_given) - 1) :\n",
        "            if(answer_given[i] in negative and answer_given[i+1] in negative) :\n",
        "                semantic_2 += 1 \n",
        "            elif(answer_given[i] in negative and answer_given[i+1] not in negative) :\n",
        "                semantic_2 -= 1 \n",
        "        \n",
        "        if(semantic_1 == 0 and semantic_2 == 0) :\n",
        "            \n",
        "            \"\"\"\n",
        "            Well and good\n",
        "            \"\"\"\n",
        "        elif(semantic_1 < 0  and semantic_2 >= 0) :\n",
        "            not_matching_semantics.append(list([actual[index],given[z]]))\n",
        "            embed_z*=(-1)\n",
        "\n",
        "        elif(semantic_1 >= 0 and semantic_2 < 0 ) :\n",
        "            not_matching_semantics.append(list([actual[index],given[z]]))\n",
        "            embed_z*=(-1)\n",
        "        \n",
        "        #print(semantic_1,semantic_2,actual[index],given[z])\n",
        "        given_embed_list.append(embed_z)\n",
        "    \n",
        "    #print(np.array(actual_embed_list).shape)\n",
        "    actual_embed = actual_embed_list[0] \n",
        "    #print(actual_embed.shape) \n",
        "    \n",
        "    for i in range(len(actual_embed_list)-1) :\n",
        "        #print(actual_embed_list[i+1].shape)\n",
        "        actual_embed += actual_embed_list[i+1]\n",
        "        \n",
        "    given_embed = given_embed_list[0] \n",
        "    for i in range(len(given_embed_list) - 1) :\n",
        "        given_embed += given_embed_list[i+1] \n",
        "            \n",
        "    actual_embed = np.array(actual_embed).reshape(512)\n",
        "    given_embed = np.array(given_embed).reshape(512) \n",
        "    sem_checker = list([actual_embed,given_embed]) \n",
        "    answer = pd.DataFrame(cos_sim(sem_checker))\n",
        "        \n",
        "    return not_matching_semantics , answer[0][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_oGCLP-tGGd"
      },
      "source": [
        "def WMD(actual_answer, given_answer, model) :\n",
        "    actual_answer = actual_answer.lower().split()\n",
        "    actual_answer = [w for w in actual_answer if w not in stop_words]\n",
        "    \n",
        "    given_answer = given_answer.lower().split()\n",
        "    given_answer = [w for w in given_answer if w not in stop_words]\n",
        "    \n",
        "    return model.wmdistance(given_answer,actual_answer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrGpKXAftR0O"
      },
      "source": [
        "def score(given_answer, actual_answer, model) :\n",
        "    given_answer1 = given_answer[:]\n",
        "    actual_answer1 = actual_answer[:]\n",
        "    \n",
        "    given_answer2 = given_answer[:]\n",
        "    actual_answer2 = actual_answer[:]\n",
        "\n",
        "    not_matching , similarity = semantic_similarity(actual_answer1, given_answer1)\n",
        "    distance = WMD(actual_answer2, given_answer2, model)\n",
        "    \n",
        "    # if(similarity > 0) :\n",
        "    # if(distance == 0) :\n",
        "    #     return 1 \n",
        "    print(\"NOT MATCHING TEXT: \", not_matching)\n",
        "    return similarity/distance\n",
        "    # else :\n",
        "        # return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qnx_1xNeSS2U"
      },
      "source": [
        "# Run with Flask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rYFnOGNS6nv"
      },
      "source": [
        "%pip install -q flask \n",
        "%pip install -q flask_ngrok \n",
        "%pip install -q werkzeug \n",
        "%pip install -q flask_wtf\n",
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/Jay22519/Automatic-Answer-checker-\n",
        "%cd Automatic-Answer-checker-\n",
        "!printf \"Currently in directory: \"; pwd\n",
        "\n",
        "print(\"Path to saved model:\", path_to_saved_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAcw6hNJSSL-"
      },
      "source": [
        "from flask import Flask, redirect, render_template, request, make_response, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from werkzeug.utils import secure_filename\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "UPLOAD_FOLDER = os.path.join(\"static\",\"assets\",\"img\", 'uploads')\n",
        "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "\n",
        "app.config['JSON_SORT_KEYS'] = False\n",
        "\n",
        "def file_name(filename):\n",
        "    return os.path.join(UPLOAD_FOLDER, filename)\n",
        "\n",
        "@app.route(\"/\")\n",
        "@app.route(\"/index\")\n",
        "def home():\n",
        "    return render_template(\"index.html\")\n",
        "\n",
        "@app.route(\"/live-demo\")\n",
        "def live_demo():\n",
        "    return render_template(\"live-demo.html\")\n",
        "\n",
        "@app.route(\"/result\", methods = ['GET', 'POST'])\n",
        "def result():\n",
        "    print(\"******* Inside RESULT **********\")\n",
        "    if request.method == 'POST':\n",
        "        print(\"IT'S POST MALONE: \", request.files)\n",
        "\n",
        "        teacher_file = request.files.get('file1', None)\n",
        "        student_files = request.files.getlist('file2', None)\n",
        "\n",
        "        # print(\"Got files: \", file1, file2)\n",
        "\n",
        "        if teacher_file and student_files:\n",
        "            teacher_file.save(file_name(teacher_file.filename))\n",
        "\n",
        "            for file_ in student_files:\n",
        "                file_.save(file_name(file_.filename))\n",
        "\n",
        "            teacher_text = get_ocr_text([file_name(teacher_file.filename)])[0]\n",
        "            students_texts = get_ocr_text([file_name(f.filename) for f in student_files])\n",
        "            \n",
        "            resulting_scores = [score(teacher_text, stud, model) for stud in students_texts]            \n",
        "            print(\"res is of length\", len(resulting_scores))\n",
        "\n",
        "            # return redirect(url_for(\"\"))\n",
        "            results_dict = {\n",
        "                \"Teacher Filepath\": file_name(teacher_file.filename),\n",
        "                \"Teacher Filename\": teacher_file.filename,\n",
        "                \"Student Grades\": [{\n",
        "                    \"Filepath\": file_name(f.filename),\n",
        "                    \"Filename\": f.filename,\n",
        "                    \"Score\": f\"{r:.4f}\",\n",
        "                } for f, r in zip(student_files, resulting_scores)],\n",
        "            }\n",
        "            return render_template(\"output.html\", result=results_dict)\n",
        "    return make_response(\"Invalid somehow!\")#redirect(url_for('home'))\n",
        "\n",
        "# @app.route(\"/results\")\n",
        "def output():\n",
        "    pass\n",
        "\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJgZq1ByO5c3"
      },
      "source": [
        "%cd /content\n",
        "%rm -r Automatic-Answer-checker-\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}